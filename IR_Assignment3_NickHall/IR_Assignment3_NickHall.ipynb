{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEjSL8GQtK4o"
      },
      "source": [
        "# Setup/Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO0C8Ms1sWYU",
        "outputId": "bd6d6467-f813-47f9-f3ab-670433c9cf37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Getting the post reader\n",
        "from post_parser_record import PostParserRecord\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
        "\n",
        "# Getting the tokenizer\n",
        "!pip install nltk \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords  \n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "\n",
        "# Cleans the text\n",
        "\n",
        "def clean_string(s):\n",
        "  # Removes HTML tags\n",
        "  CLEANR = re.compile('<.*?>') \n",
        "  s = re.sub(CLEANR, '', s)\n",
        "  # Removes newlines and adds a space so that words do not combine\n",
        "  s = s.replace('\\n', ' ') \n",
        "  # Removes punctuation\n",
        "  s = s.translate(str.maketrans('','',string.punctuation))\n",
        "  \n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur3bFBI2tcDN"
      },
      "source": [
        "# Creating Index for the three models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF & BM25"
      ],
      "metadata": {
        "id": "gA6UFUg9ewwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "fiZ0GUqvtDq8"
      },
      "outputs": [],
      "source": [
        "# Creates the Inverted Index\n",
        "inverted_index = {}\n",
        "tf_index = {}\n",
        "tf_ind = {}\n",
        "tf_idf_index= {}\n",
        "c = len(post_reader.map_questions) + len(post_reader.map_just_answers) # Total doc count\n",
        "avg = 0\n",
        "doc_length = {}\n",
        "# Gathering Question Data\n",
        "for question_id in post_reader.map_questions:\n",
        "  doc_freq = {}\n",
        "\n",
        "  # Gets question\n",
        "  question = post_reader.map_questions[question_id]\n",
        "\n",
        "  # Combines question body and title\n",
        "  question_b = question.body.lower().strip()\n",
        "  question_t = question.title.lower().strip()\n",
        "  question_bt = question_b + \" \" + question_t\n",
        "\n",
        "  # Cleaning\n",
        "  question_bt = clean_string(question_bt)\n",
        "\n",
        "  # Tokenizing\n",
        "  token_words = nltk.word_tokenize(question_bt)\n",
        "  token_words = [w for w in token_words if not w.lower() in stop_words]\n",
        "  l = len(token_words)\n",
        "  \n",
        "  avg += l # To store avg length of all docs\n",
        "  doc_length.update({question_id: l})  # Used for BM25\n",
        "\n",
        "  for token in token_words:   # Loop through tokens\n",
        "    if token in inverted_index:     # Token is already in index\n",
        "      doc_freq = inverted_index[token]  # Sets doc_freq to the current postings/counts for the token\n",
        "      if question_id in doc_freq:\n",
        "        freq = doc_freq[question_id] + 1\n",
        "        doc_freq.update({question_id: freq})\n",
        "        inverted_index.update({token: doc_freq})\n",
        "      else:\n",
        "        doc_freq.update({question_id: 1})\n",
        "    else:   # Token is not in index, adds to index with a frequency of 1\n",
        "      doc_freq = {question_id: 1}    \n",
        "      inverted_index.update({token: doc_freq})\n",
        "\n",
        "      # Calculates TF \n",
        "  for token in token_words: \n",
        "    if token in tf_index:\n",
        "      tf_ind = tf_index[token]\n",
        "      tf = (inverted_index[token][question_id])/l\n",
        "      tf_ind.update({question_id: tf})\n",
        "      tf_index.update({token: tf_ind})\n",
        "    else:\n",
        "      tf_ind = {}\n",
        "      tf = (inverted_index[token][question_id])/l\n",
        "      tf_ind.update({question_id: tf})\n",
        "      tf_index.update({token: tf_ind})\n",
        "\n",
        "# Gathering Answer Data\n",
        "for answer_id in post_reader.map_just_answers:\n",
        "  doc_freq = {}\n",
        "\n",
        "  # Gets answer and text\n",
        "  answer = post_reader.map_just_answers[answer_id]\n",
        "  answer_text = answer.body.lower().strip()\n",
        "\n",
        "  # Cleaning\n",
        "  answer_text = clean_string(answer_text)\n",
        "\n",
        "  # Tokenizing\n",
        "  token_words2 = nltk.word_tokenize(answer_text)\n",
        "  token_words2 = [w for w in token_words2 if not w.lower() in stop_words]\n",
        "  l = len(token_words2)\n",
        "  \n",
        "  avg += l # To store avg length of all docs\n",
        "  doc_length.update({answer_id: l})  # Used for BM25\n",
        "\n",
        "  for token in token_words2:   # Loop through tokens\n",
        "    if token in inverted_index:     # Token is already in index\n",
        "      doc_freq = inverted_index[token]  \n",
        "      if answer_id in doc_freq:\n",
        "        freq = doc_freq[answer_id] + 1\n",
        "        doc_freq.update({answer_id: freq})\n",
        "        inverted_index.update({token: doc_freq})\n",
        "      else:\n",
        "        doc_freq.update({answer_id: 1})\n",
        "    else:   # Token is not in index, adds to index with a frequency of 1\n",
        "      doc_freq = {answer_id: 1}     \n",
        "      inverted_index.update({token: doc_freq})\n",
        "\n",
        "     # Calculates TF \n",
        "  for token in token_words2: \n",
        "    if token in tf_index:\n",
        "      tf_ind = tf_index[token]\n",
        "      tf = (inverted_index[token][answer_id])/l\n",
        "      tf_ind.update({answer_id: tf})\n",
        "      tf_index.update({token: tf_ind})\n",
        "    else:\n",
        "      tf_ind = {}\n",
        "      tf = (inverted_index[token][answer_id])/l\n",
        "      tf_ind.update({answer_id: tf})\n",
        "      tf_index.update({token: tf_ind})\n",
        "\n",
        "# Calculates the average words\n",
        "avg = avg/c\n",
        "# Sorting keys \n",
        "sorted_keys = sorted(inverted_index.keys())\n",
        "inverted_index = {key:inverted_index[key] for key in sorted_keys}\n",
        "\n",
        "# Sorting values\n",
        "for word in inverted_index:\n",
        "  inverted_index[word] = dict(sorted(inverted_index[word].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Calculates TF-IDF\n",
        "for word in inverted_index:\n",
        "  tf_idf = {}\n",
        "  for id in tf_index[word]:\n",
        "    IDF = math.log(c /len(inverted_index[word]))\n",
        "    tf_idf_score = IDF * tf_index[word][id]\n",
        "    tf_idf.update({id: tf_idf_score})\n",
        "    tf_idf_index.update({word : tf_idf})\n",
        "\n",
        "# Sorting values of tf_idf_index\n",
        "for word in tf_idf_index:\n",
        "  tf_idf_index[word] = dict(sorted(tf_idf_index[word].items(), key=lambda item: item[1], reverse=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VSM"
      ],
      "metadata": {
        "id": "cPPk_2Rge7HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst_terms = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# First making temp dict for making vectors\n",
        "temp_dic = {}\n",
        "count = 0\n",
        "\n",
        "# Looping through Questions for text and input into temp dic\n",
        "for question_id in post_reader.map_questions:\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  text = question.body.strip() + \" \" + question.title.strip()\n",
        "  text = text.lower()\n",
        "  text = clean_string(text)\n",
        "\n",
        "  filtered_words = nltk.word_tokenize(text)\n",
        "  filtered_words = [w for w in filtered_words if not w.lower() in stop_words]\n",
        "\n",
        "  lst_terms.extend(filtered_words)\n",
        "\n",
        "for term in set(lst_terms):\n",
        "  if term not in temp_dic:\n",
        "    temp_dic[term] = count\n",
        "    count += 1\n",
        "\n",
        "# Looping through Answers for text and input into temp dic\n",
        "for ans_id in post_reader.map_just_answers:\n",
        "  answer = post_reader.map_just_answers[ans_id]\n",
        "  text = answer.body.strip()\n",
        "  text = text.lower()\n",
        "  text = clean_string(text)\n",
        "\n",
        "  filtered_words = nltk.word_tokenize(text)\n",
        "  filtered_words = [w for w in filtered_words if not w.lower() in stop_words]\n",
        "\n",
        "  lst_terms.extend(filtered_words)\n",
        "\n",
        "for term in set(lst_terms):\n",
        "  if term not in temp_dic:\n",
        "    temp_dic[term] = count\n",
        "    count += 1\n",
        "\n",
        "# dict of question id/answer id as the key and doc vector as value\n",
        "dic_doc_vector = {}\n",
        "\n",
        "# Loop for questions\n",
        "for question_id in post_reader.map_questions:\n",
        "  # Cleaning the text up and tokenizing\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  doc_vector = [0] * len(temp_dic.keys())\n",
        "  text = question.body.strip() + \" \" + question.title.strip()\n",
        "  text = text.lower()\n",
        "  text = clean_string(text)\n",
        "\n",
        "  filtered_words = nltk.word_tokenize(text)\n",
        "  filtered_words = [w for w in filtered_words if not w.lower() in stop_words]\n",
        "\n",
        "  # Loop through tokens setting the index of the doc vector to the corresponding tfidf value for the term\n",
        "  for token in filtered_words:\n",
        "    if token in temp_dic:\n",
        "      index = temp_dic[token]\n",
        "      doc_vector[index] = tf_idf_index[token][question_id]\n",
        "  dic_doc_vector[question_id] = doc_vector\n",
        "\n",
        "# Loop for answers\n",
        "for ans_id in post_reader.map_just_answers:\n",
        "  # Cleaning the text up and tokenizing\n",
        "  answer = post_reader.map_just_answers[ans_id]\n",
        "  doc_vector = [0] * len(temp_dic.keys())\n",
        "  text = answer.body\n",
        "  text = text.lower()\n",
        "  text = clean_string(text)\n",
        "\n",
        "  filtered_words = nltk.word_tokenize(text)\n",
        "  filtered_words = [w for w in filtered_words if not w.lower() in stop_words]\n",
        "\n",
        "  # Loop through tokens setting the index of the doc vector to the corresponding tfidf value for the term\n",
        "  for token in filtered_words:\n",
        "    if token in temp_dic:\n",
        "      index = temp_dic[token]\n",
        "      doc_vector[index] = tf_idf_index[token][ans_id]\n",
        "  dic_doc_vector[ans_id] = doc_vector"
      ],
      "metadata": {
        "id": "W7_hZRQDfKmj"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgmmPYW0tfxR"
      },
      "source": [
        "# Implementation of 3 retrieval models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method to print top k ranked results"
      ],
      "metadata": {
        "id": "rtDi0xk3oWNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to print the first k search results by rank. For this project I set k to 50\n",
        "def top_k(results, k):\n",
        "\n",
        "  if len(results)==0:\n",
        "    print(\"No results\")\n",
        "    return\n",
        "  top_k = results[:k]\n",
        "  rank = 1\n",
        "  print(\"Rank\\tPost ID\")\n",
        "  for item in top_k:\n",
        "    print(str(rank) + \"\\t\" + str(item))\n",
        "    rank+=1"
      ],
      "metadata": {
        "id": "O7WRxdZbQdkv"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLv0X8c_tl-C"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "97csurES_ace"
      },
      "outputs": [],
      "source": [
        "def retrieve_by_tfidf(dict1, dict2):\n",
        "  # These are doc lists in sorted order\n",
        "  list1 = list(sorted(dict1))\n",
        "  list2 = list(sorted(dict2))\n",
        "  doc_count = {}\n",
        "  matches = []\n",
        "  i = 0\n",
        "  j = 0\n",
        "  count = 0\n",
        "\n",
        "  # Loop to run through each doc for first term\n",
        "  while i < len(list1) and j < len(list2):\n",
        "      if list1[i] < list2[j]:\n",
        "        count = dict1[list1[i]]\n",
        "        doc_count.update({list1[i]: count})\n",
        "        matches.append(list1[i])\n",
        "        i += 1\n",
        "      elif list2[j] < list1[i]:\n",
        "        count = dict2[list2[j]]\n",
        "        doc_count.update({list2[j]: count})\n",
        "        j += 1\n",
        "      else:\n",
        "        count = dict2[list2[j]] + dict1[list1[i]]\n",
        "        doc_count.update({list2[j]: count})\n",
        "        i += 1\n",
        "        j += 1\n",
        "  \n",
        "  # Add rest of list1 to matches\n",
        "  while i < len(list1):\n",
        "      count = dict1[list1[i]]\n",
        "      doc_count.update({list1[i]: count})\n",
        "      i += 1\n",
        "  \n",
        "  # Add rest of list2 to matches\n",
        "  while j < len(list2):\n",
        "      count = dict2[list2[j]]\n",
        "      doc_count.update({list2[j]: count})\n",
        "      j += 1\n",
        "\n",
        "  # Sorts by tf-idf\n",
        "  doc_count = dict(sorted(doc_count.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "  return doc_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "vJPkFNWnt1C4"
      },
      "outputs": [],
      "source": [
        "# Function to do retrieval, ranking by frequency of the words in the document\n",
        "def tfidf_search (query):\n",
        "  # Breaks up search terms\n",
        "  query = clean_string(query)\n",
        "  search_terms = query.lower().split()\n",
        "  terms = []\n",
        "  \n",
        "  # Case for all inputs not in index\n",
        "  for term in search_terms:\n",
        "    if term in tf_idf_index:\n",
        "      break\n",
        "    elif term not in tf_idf_index and term != search_terms[len(search_terms) - 1]:\n",
        "      search_terms.remove(term)\n",
        "    elif term == search_terms[len(search_terms) - 1]:\n",
        "      return []\n",
        "  \n",
        "  # Case for looking at inputs\n",
        "  for term in search_terms:\n",
        "    if term in tf_idf_index:\n",
        "      terms.append(term)\n",
        "\n",
        "  # If search terms is empty returns no results\n",
        "  if len(terms) == 0:\n",
        "    return []\n",
        "\n",
        "  # Single search\n",
        "  if len(terms) == 1:\n",
        "    return list(tf_idf_index[terms[0]].keys())\n",
        "  \n",
        "  else: # Search for more than 1 term\n",
        "    matches = []\n",
        "    temp_dic = retrieve_by_tfidf(tf_idf_index[terms[0]], tf_idf_index[terms[1]])\n",
        "    terms.remove(terms[0])\n",
        "    terms.remove(terms[0])\n",
        "    \n",
        "    if len(terms) == 0:\n",
        "      return list(temp_dic.keys())\n",
        "    else:\n",
        "      for term in terms:\n",
        "        temp_dic = retrieve_by_tfidf(temp_dic, tf_idf_index[term])\n",
        "\n",
        "      # Sorting and converting to list for output\n",
        "      temp_dic = dict(sorted(temp_dic.items(), key=lambda item: item[1], reverse=True))\n",
        "      matches = list(temp_dic.keys())\n",
        "      return matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_QHGyTtpk-"
      },
      "source": [
        "### VSM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to speed up computing the cosine simularity calculation\n",
        "\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "@jit(nopython=True)\n",
        "def cosine_similarity_numba(u:np.ndarray, v:np.ndarray):\n",
        "    assert(u.shape[0] == v.shape[0])\n",
        "    uv = 0\n",
        "    uu = 0\n",
        "    vv = 0\n",
        "    for i in range(u.shape[0]):\n",
        "        uv += u[i]*v[i]\n",
        "        uu += u[i]*u[i]\n",
        "        vv += v[i]*v[i]\n",
        "    cos_theta = 1\n",
        "    if uu!=0 and vv!=0:\n",
        "        cos_theta = uv/np.sqrt(uu*vv)\n",
        "    return cos_theta"
      ],
      "metadata": {
        "id": "_Il0nsR_xO7Z"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "eD_uqiG-t1ZY"
      },
      "outputs": [],
      "source": [
        "def vsm (query):\n",
        "  search_terms = [] # List of search terms\n",
        "  term_freq = {}  # Dictionary holding the term and frequency of the query terms\n",
        "  length = len(query.split()) # Length of the query\n",
        "  results = {} # Resulting dictionary holding doc and cosign simularity\n",
        "\n",
        "  # Breaks up search terms\n",
        "  query = clean_string(query)\n",
        "  for term in query.split():\n",
        "    if term in lst_terms:\n",
        "      if term in search_terms:\n",
        "        freq = term_freq[term] + 1\n",
        "        term_freq.update({term: freq})\n",
        "      else:\n",
        "        term_freq.update({term : 1})\n",
        "      search_terms.append(term)\n",
        "\n",
        "  # Creates empty query vector\n",
        "  query_vector = [0] * len(temp_dic.keys())\n",
        "\n",
        "  # Loop through terms making query vector index equal to corresponding tfidf value\n",
        "  for term in search_terms:\n",
        "    if term in temp_dic:\n",
        "      IDF = math.log(c /len(inverted_index[term]))\n",
        "      tf_idf_score = IDF * (term_freq[term] / length)\n",
        "      index = temp_dic[term]\n",
        "      query_vector[index] = tf_idf_score\n",
        "  \n",
        "  # Caculates Cosign simularity between query vector and each foc\n",
        "  b = query_vector\n",
        "  for doc in dic_doc_vector:\n",
        "    a = dic_doc_vector[doc]\n",
        "    cos_sim = cosine_similarity_numba(np.array(a),np.array(b))\n",
        "    results.update({doc : cos_sim})\n",
        "  \n",
        "  # Sorting\n",
        "  results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))\n",
        "  matches = list(results.keys())\n",
        "  return matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mdG6xwWtvNR"
      },
      "source": [
        "### BM25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25 (query):\n",
        "  search_terms = []\n",
        "\n",
        "  # Breaks up search terms\n",
        "  query = clean_string(query)\n",
        "  for term in set(query.split()):\n",
        "    if term in lst_terms:\n",
        "      search_terms.append(term)\n",
        "  \n",
        "  temp_dict = {}\n",
        "  query_dict = {}\n",
        "  result_dict = {}\n",
        "  doc_set = set()\n",
        "\n",
        "  # Calculation and making a dict\n",
        "  for term in search_terms:\n",
        "    for doc in tf_idf_index[term]:\n",
        "      doc_set.add(doc)\n",
        "      if doc in temp_dict:\n",
        "        score = temp_dict[doc] + calc_bm25(term, doc)\n",
        "        temp_dict.update({doc : score})\n",
        "      else:\n",
        "        score = calc_bm25(term, doc)\n",
        "        temp_dict.update({doc : score})\n",
        "    query_dict.update({term : temp_dict})\n",
        "\n",
        "  # Single search\n",
        "  if len(search_terms) == 1:\n",
        "    dic = query_dict[search_terms[0]]\n",
        "    dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse=True))\n",
        "    return list(dic.keys())\n",
        "\n",
        "  # Multi search\n",
        "  # Gets the score for each term in each doc and then adds the score and stores in a dictionary mapping of doc to overall query score\n",
        "  for term in search_terms:\n",
        "    for doc in doc_set:\n",
        "      if doc in query_dict[term]:\n",
        "        if doc in result_dict:\n",
        "          score = result_dict[doc] + query_dict[term][doc]\n",
        "          result_dict.update({doc : score})\n",
        "        else:\n",
        "          result_dict.update({doc : score})\n",
        "\n",
        "  result_dict = dict(sorted(result_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "  matches = list(result_dict.keys())\n",
        "  return matches"
      ],
      "metadata": {
        "id": "3posyfBelvKh"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method to calculate and return the value needed for BM25 model\n",
        "def calc_bm25 (term, doc):\n",
        "  # Hyperparameters\n",
        "  k = 1.2\n",
        "  b = 0.75\n",
        "  # Calculation\n",
        "  idf = tf_idf_index[term][doc]\n",
        "  partial_score = ((k + 1) * inverted_index[term][doc]) / (k * ((1 - b) + b * (doc_length[doc]/avg)) + inverted_index[term][doc])\n",
        "  score = idf * partial_score\n",
        "  return score"
      ],
      "metadata": {
        "id": "uqJoah8uwbuO"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPkpWAQQt15s"
      },
      "source": [
        "# Testing of each model for given queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7sLethAt-Mc"
      },
      "source": [
        "#### a. espresso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "DKmwA3HfuRCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1593d33c-b485-49e1-f2f6-df47f8efe817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Search\n",
            "Rank\tPost ID\n",
            "1\t4404\n",
            "2\t2867\n",
            "3\t3168\n",
            "4\t3904\n",
            "5\t3800\n",
            "VSM Search\n",
            "Rank\tPost ID\n",
            "1\t2116\n",
            "2\t5528\n",
            "3\t470\n",
            "4\t466\n",
            "5\t2095\n",
            "BM25 Search\n",
            "Rank\tPost ID\n",
            "1\t4404\n",
            "2\t2867\n",
            "3\t3168\n",
            "4\t4258\n",
            "5\t3800\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF Search\")\n",
        "top_k(tfidf_search(\"espresso\"), 5)\n",
        "print(\"VSM Search\")\n",
        "top_k(vsm(\"espresso\"), 5)\n",
        "print(\"BM25 Search\")\n",
        "top_k(bm25(\"espresso\"), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRiNyvYBuCAu"
      },
      "source": [
        "#### b. turkish coffee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "AqT7oDLXuRrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c28c96-d6b1-4803-8f0a-67a32b3e3c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Search\n",
            "Rank\tPost ID\n",
            "1\t5094\n",
            "2\t5182\n",
            "3\t483\n",
            "4\t209\n",
            "5\t3074\n",
            "VSM Search\n",
            "Rank\tPost ID\n",
            "1\t5094\n",
            "2\t209\n",
            "3\t2394\n",
            "4\t3074\n",
            "5\t483\n",
            "BM25 Search\n",
            "Rank\tPost ID\n",
            "1\t5094\n",
            "2\t5182\n",
            "3\t483\n",
            "4\t209\n",
            "5\t3074\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF Search\")\n",
        "top_k(tfidf_search(\"turkish coffee\"), 5)\n",
        "print(\"VSM Search\")\n",
        "top_k(vsm(\"turkish coffee\"), 5)\n",
        "print(\"BM25 Search\")\n",
        "top_k(bm25(\"turkish coffee\"), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CuhU41GuHLf"
      },
      "source": [
        "#### c. making a decaffeinated coffee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "UWz3dTTVuR3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20b3500-04db-43d3-83d5-496aa6a0760d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Search\n",
            "Rank\tPost ID\n",
            "1\t204\n",
            "2\t120\n",
            "3\t3293\n",
            "4\t2897\n",
            "5\t373\n",
            "VSM Search\n",
            "Rank\tPost ID\n",
            "1\t120\n",
            "2\t204\n",
            "3\t2897\n",
            "4\t2555\n",
            "5\t373\n",
            "BM25 Search\n",
            "Rank\tPost ID\n",
            "1\t204\n",
            "2\t120\n",
            "3\t3293\n",
            "4\t2897\n",
            "5\t373\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF Search\")\n",
        "top_k(tfidf_search(\"making a decaffeinated coffee\"), 5)\n",
        "print(\"VSM Search\")\n",
        "top_k(vsm(\"making a decaffeinated coffee\"), 5)\n",
        "print(\"BM25 Search\")\n",
        "top_k(bm25(\"making a decaffeinated coffee\"), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcXARzIUuKpz"
      },
      "source": [
        "#### d. can I use the same coffee grounds twice?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "F-goKS_0uSE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3fc8c2-f17b-4635-cefa-9069ede13950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Search\n",
            "Rank\tPost ID\n",
            "1\t2683\n",
            "2\t1749\n",
            "3\t3258\n",
            "4\t3966\n",
            "5\t183\n",
            "VSM Search\n",
            "Rank\tPost ID\n",
            "1\t2683\n",
            "2\t1749\n",
            "3\t3258\n",
            "4\t3966\n",
            "5\t4087\n",
            "BM25 Search\n",
            "Rank\tPost ID\n",
            "1\t2683\n",
            "2\t3258\n",
            "3\t1749\n",
            "4\t183\n",
            "5\t2585\n"
          ]
        }
      ],
      "source": [
        "print(\"TF-IDF Search\")\n",
        "top_k(tfidf_search(\"can I use the same coffee grounds twice?\"), 5)\n",
        "print(\"VSM Search\")\n",
        "top_k(vsm(\"can I use the same coffee grounds twice?\"), 5)\n",
        "print(\"BM25 Search\")\n",
        "top_k(bm25(\"can I use the same coffee grounds twice?\"), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table"
      ],
      "metadata": {
        "id": "Aq-xPglfac9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = tfidf_search(\"espresso\")\n",
        "b1 = vsm(\"espresso\")\n",
        "c1 = bm25(\"espresso\")\n",
        "a2 = tfidf_search(\"turkish coffee\")\n",
        "b2 = vsm(\"turkish coffee\")\n",
        "c2 = bm25(\"turkish coffee\")\n",
        "a3 = tfidf_search(\"making a decaffeinated coffee\")\n",
        "b3 = vsm(\"making a decaffeinated coffee\")\n",
        "c3 = bm25(\"making a decaffeinated coffee\")\n",
        "a4 = tfidf_search(\"can I use the same coffee grounds twice?\")\n",
        "b4 = vsm(\"can I use the same coffee grounds twice?\")\n",
        "c4 = bm25(\"can I use the same coffee grounds twice?\")"
      ],
      "metadata": {
        "id": "cf2H_NfmcD6L"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "myTable = PrettyTable([\"Query\", \"TF-IDF\", \"VSM\", \"BM25\"])\n",
        "\n",
        "# Adds rows\n",
        "myTable.add_row([\"espresso\", a1[:5], b1[:5], c1[:5]])\n",
        "myTable.add_row([\"turkish coffee\", a2[:5], b2[:5], c2[:5]])\n",
        "myTable.add_row([\"making a decaffeinated coffee\", a3[:5], b3[:5], c3[:5]])\n",
        "myTable.add_row([\"can I use the same coffee grounds twice?\", a4[:5], b4[:5], c4[:5]])\n",
        "\n",
        "print(myTable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQo9r0m8afII",
        "outputId": "4c54464a-3111-42db-eb18-73355edb4320"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
            "|                  Query                   |             TF-IDF             |              VSM               |              BM25              |\n",
            "+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
            "|                 espresso                 | [4404, 2867, 3168, 3904, 3800] |  [2116, 5528, 470, 466, 2095]  | [4404, 2867, 3168, 4258, 3800] |\n",
            "|              turkish coffee              |  [5094, 5182, 483, 209, 3074]  |  [5094, 209, 2394, 3074, 483]  |  [5094, 5182, 483, 209, 3074]  |\n",
            "|      making a decaffeinated coffee       |  [204, 120, 3293, 2897, 373]   |  [120, 204, 2897, 2555, 373]   |  [204, 120, 3293, 2897, 373]   |\n",
            "| can I use the same coffee grounds twice? | [2683, 1749, 3258, 3966, 183]  | [2683, 1749, 3258, 3966, 4087] | [2683, 3258, 1749, 183, 2585]  |\n",
            "+------------------------------------------+--------------------------------+--------------------------------+--------------------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}